name: CI/CD

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  check-changes:
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    outputs:
      test_data: ${{ steps.filter.outputs.test_data }}
      frontend: ${{ steps.filter.outputs.frontend }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            test_data:
              - 'test_data/**'
              - 'extraction_service/train_*.py'
              - 'extraction_service/extract_*.py'
            frontend:
              - 'frontend/**'

  validate-test-data:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Validate ground truth (warn-only for known issues)
        run: python test_data/validate_ground_truth.py --warn-only

      - name: Validate cage contiguity
        run: python test_data/validate_cage_contiguity.py

  test-backend:
    runs-on: ubuntu-latest
    needs: validate-test-data
    defaults:
      run:
        working-directory: backend
    steps:
      - uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.21'

      - name: Run tests
        run: go test ./...

  build-backend:
    runs-on: ubuntu-latest
    needs: test-backend
    defaults:
      run:
        working-directory: backend
    steps:
      - uses: actions/checkout@v4

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.21'

      - name: Build
        run: GOOS=linux GOARCH=amd64 go build -o sudoku-server .

      - name: Upload backend artifact
        uses: actions/upload-artifact@v4
        with:
          name: backend-binary
          path: backend/sudoku-server

  build-frontend:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: frontend
    steps:
      - uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: frontend/package-lock.json

      - name: Install dependencies
        run: npm ci

      - name: Build
        run: npm run build

      - name: Upload frontend artifact
        uses: actions/upload-artifact@v4
        with:
          name: frontend-dist
          path: frontend/dist

  retrain-models:
    runs-on: ubuntu-latest
    needs: [test-backend, validate-test-data, check-changes]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push' && needs.check-changes.outputs.test_data == 'true'
    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: extraction_service/requirements.txt

      - name: Install dependencies
        run: |
          cd extraction_service
          pip install -r requirements.txt

      - name: Extract CNN training data from test images
        run: |
          cd extraction_service
          echo "Extracting training cells from test_data images..."
          python extract_training_cells.py
          echo "Training data extraction complete"

      - name: Train Digit CNN
        run: |
          cd extraction_service
          echo "Training Digit CNN classifier..."
          python train_digit_cnn.py --epochs 20 --batch-size 32
          echo "Digit CNN training complete"

      - name: Extract boundary training data
        run: |
          cd extraction_service
          echo "Extracting boundary training data..."
          python extract_boundary_training_data.py

      - name: Train Boundary Classifier
        run: |
          cd extraction_service
          echo "Training boundary classifier..."
          python train_boundary_classifier.py
          echo "Boundary classifier training complete"

      - name: Extract cage sum training data
        run: |
          cd extraction_service
          echo "Extracting cage sum training data..."
          python extract_cage_sum_training_data.py
          echo "Cage sum data extraction complete"

      - name: Prepare cage sum CNN data
        run: |
          cd extraction_service
          echo "Preparing cage sum CNN data..."
          python prepare_cage_sum_cnn_data.py
          echo "Cage sum CNN data preparation complete"

      - name: Train Cage Sum CNN
        run: |
          cd extraction_service
          echo "Training Cage Sum CNN..."
          python train_cage_sum_cnn.py --epochs 30 --batch-size 32
          echo "Cage Sum CNN training complete"

      - name: Upload retrained models
        uses: actions/upload-artifact@v4
        with:
          name: ml-models
          path: |
            extraction_service/models/digit_cnn.pth
            extraction_service/models/boundary_classifier_rf.pkl
            extraction_service/models/boundary_scaler.pkl
            extraction_service/models/cage_sum_cnn.pth

      - name: Display model info
        run: |
          echo "Retrained ML Models:"
          ls -lh extraction_service/models/

  deploy:
    runs-on: self-hosted
    needs: [build-backend, build-frontend, retrain-models, check-changes]
    if: |
      always() &&
      github.ref == 'refs/heads/main' &&
      github.event_name == 'push' &&
      needs.build-backend.result == 'success' &&
      needs.build-frontend.result == 'success' &&
      (needs.retrain-models.result == 'success' || needs.retrain-models.result == 'skipped')
    environment: production
    env:
      SERVER: "192.168.86.227"
      APP_DIR: "/opt/sudoku"
      SERVICE_NAME: "sudoku"
      EXTRACTION_SERVICE_NAME: "sudoku-extraction"
    steps:
      - uses: actions/checkout@v4

      - name: Download backend artifact
        uses: actions/download-artifact@v4
        with:
          name: backend-binary
          path: backend

      - name: Download frontend artifact
        uses: actions/download-artifact@v4
        with:
          name: frontend-dist
          path: frontend/dist

      - name: Download ML models artifact (from this run)
        if: needs.retrain-models.result == 'success'
        uses: actions/download-artifact@v4
        with:
          name: ml-models
          path: extraction_service/models

      - name: Download ML models artifact (from previous run)
        if: needs.retrain-models.result == 'skipped'
        uses: dawidd6/action-download-artifact@v6
        with:
          workflow: ci-cd.yml
          name: ml-models
          path: extraction_service/models
          search_artifacts: true
          if_no_artifact_found: fail

      - name: Make backend binary executable
        run: chmod +x backend/sudoku-server

      - name: Stop services
        run: |
          ssh $SERVER "sudo systemctl stop $SERVICE_NAME || true"
          ssh $SERVER "sudo systemctl stop $EXTRACTION_SERVICE_NAME || true"

      - name: Prepare remote directory
        run: |
          ssh $SERVER "sudo mkdir -p $APP_DIR && sudo chown \$(whoami) $APP_DIR"
          ssh $SERVER "mkdir -p $APP_DIR/extraction_service"

      - name: Transfer files
        run: |
          scp backend/sudoku-server $SERVER:$APP_DIR/
          scp -r frontend/dist $SERVER:$APP_DIR/
          scp extraction_service/app.py $SERVER:$APP_DIR/extraction_service/
          scp extraction_service/digit_classifier.py $SERVER:$APP_DIR/extraction_service/
          scp extraction_service/requirements.txt $SERVER:$APP_DIR/extraction_service/

      - name: Deploy ML models
        run: |
          echo "Deploying ML models to production..."
          ssh $SERVER "mkdir -p $APP_DIR/extraction_service/models"
          scp extraction_service/models/digit_cnn.pth $SERVER:$APP_DIR/extraction_service/models/
          scp extraction_service/models/boundary_classifier_rf.pkl $SERVER:$APP_DIR/extraction_service/models/
          scp extraction_service/models/boundary_scaler.pkl $SERVER:$APP_DIR/extraction_service/models/
          scp extraction_service/models/cage_sum_cnn.pth $SERVER:$APP_DIR/extraction_service/models/
          echo "ML models deployed successfully"

      - name: Setup environment variables
        run: |
          ssh $SERVER "echo GOOGLE_API_KEY=\$(cat /home/cody/gemini-api-key) > $APP_DIR/config.env"

      - name: Setup Python environment
        run: |
          ssh $SERVER "cd $APP_DIR/extraction_service && python3 -m venv venv"
          ssh $SERVER "cd $APP_DIR/extraction_service && ./venv/bin/pip install --upgrade pip"
          ssh $SERVER "cd $APP_DIR/extraction_service && ./venv/bin/pip install -r requirements.txt"

      - name: Setup systemd services
        run: |
          scp deployment/sudoku.service $SERVER:/tmp/
          scp deployment/sudoku-extraction.service $SERVER:/tmp/
          ssh $SERVER "sudo mv /tmp/sudoku.service /etc/systemd/system/"
          ssh $SERVER "sudo mv /tmp/sudoku-extraction.service /etc/systemd/system/"
          ssh $SERVER "sudo systemctl daemon-reload"
          ssh $SERVER "sudo systemctl enable $SERVICE_NAME"
          ssh $SERVER "sudo systemctl enable $EXTRACTION_SERVICE_NAME"

      - name: Start extraction service
        run: |
          ssh $SERVER "sudo systemctl start $EXTRACTION_SERVICE_NAME"
          sleep 5

      - name: Verify extraction service
        run: |
          echo "Checking extraction service health endpoint..."
          ssh $SERVER "curl -s http://127.0.0.1:5001/health" | grep -q "ok" || echo "Warning: Extraction service may not be running properly"

          echo "Verifying ML models exist on server..."
          ssh $SERVER "test -f $APP_DIR/extraction_service/models/digit_cnn.pth || { echo 'ERROR: digit_cnn.pth not found'; exit 1; }"
          ssh $SERVER "test -f $APP_DIR/extraction_service/models/boundary_classifier_rf.pkl || { echo 'ERROR: boundary_classifier_rf.pkl not found'; exit 1; }"
          ssh $SERVER "test -f $APP_DIR/extraction_service/models/boundary_scaler.pkl || { echo 'ERROR: boundary_scaler.pkl not found'; exit 1; }"
          ssh $SERVER "test -f $APP_DIR/extraction_service/models/cage_sum_cnn.pth || { echo 'ERROR: cage_sum_cnn.pth not found'; exit 1; }"
          echo "All ML models verified on production server"

      - name: Start main service
        run: |
          ssh $SERVER "sudo systemctl start $SERVICE_NAME"

      - name: Configure nginx
        run: |
          scp deployment/sudoku.seavey.dev.conf $SERVER:/tmp/
          ssh $SERVER "sudo mv /tmp/sudoku.seavey.dev.conf /etc/nginx/sites-available/sudoku.seavey.dev.conf"
          ssh $SERVER "sudo ln -sf /etc/nginx/sites-available/sudoku.seavey.dev.conf /etc/nginx/sites-enabled/"
          ssh $SERVER "sudo nginx -t && sudo systemctl reload nginx"

      - name: Purge Cloudflare cache
        if: needs.check-changes.outputs.frontend == 'true'
        run: |
          curl -X POST "https://api.cloudflare.com/client/v4/zones/${{ secrets.CLOUDFLARE_ZONE_ID }}/purge_cache" \
            -H "Authorization: Bearer ${{ secrets.CLOUDFLARE_API_TOKEN }}" \
            -H "Content-Type: application/json" \
            --data '{"hosts":["sudoku.seavey.dev"]}'

      - name: Verify deployment
        run: |
          echo "Checking service status..."
          ssh $SERVER "sudo systemctl status $SERVICE_NAME --no-pager | head -5"
          ssh $SERVER "sudo systemctl status $EXTRACTION_SERVICE_NAME --no-pager | head -5"
